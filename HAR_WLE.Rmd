---
# Get YAML keywords from myYAML_ref.Rmd
title: "Human Activity Recognition (HAR): Weight Lifting Exercises (WLE) Classification"
author: "bdanalytics"
#output: html_document
---
#### Date: `r format(Sys.time(), "(%a) %b %d, %Y")`

Data: Measurement of exercises performed by six male participants aged between 20-28 years, with little weight lifting experience using a light dumbbell (1.25kg)  
Source: http://groupware.les.inf.puc-rio.br/har  
Time period: Nov - Dec 2011 (based on cvtd_timestamp values in data)

### Synopsis:

#### Model Summary: Random Forests:

#### How you built your model:
1. Selected Random Forests (rf) since this is a multi-class classification problem & the associated research paper indicated that the original data collectors also utilized this algorithm.

2. Optimized fitting the model (first run on training data took a long time & did not complete) by  
    2.1 Partitioned training data into training & validation data (80% - 20% split - similar to training-testing split)  
    2.2 Selected subset of training data - both rows & columns to fit "base" model within 30 elapsed seconds  
    2.3 Identified "base" model parameters such as pre-processing type, sampling method, number of resamples, mtry (rf tuning parameter) etc.  
    2.4 Generated a model parameter grid to vary parameters from the "base" model specifications  
    2.5 Fit one model for each row in the model parameter grid & collected elapsed time and prediction accuracy on the validation data set.  
    2.6 Optimized trade-off between elapsed time & validation data prediction accuracy to fit the "tuned" model within 60 elapsed seconds. This turned out to include only half the potential features and 20% of training data   
    
3. Enhanced model to fit all training (incl. validation) data & tested prediction accuracy on testing data

4. Enhanced model to fit all training (incl. validation & testing) data & generated predictions on new data

#### How you used cross validation:
1. Utilized caret's in-built sampling methods to select observations and features for each tree in the random forest

2. Sampled observations to optimize model fitting elapsed time vs. validation data prediction accuracy

#### What you think the expected out of sample error is:
Very low (compared to other machine learning exercises) since the random forests generated near-zero prediction mistakes on the validation data set. Prediction accuracy on testing data was ???. 

#### Why you made the choices you did:
Explained classification algorithm selection earlier. Most of the effort went into optimizing the trade-offs between fitting time and prediction accuracy.

#### Use your prediction model to predict 20 different test cases:
Brief summary of the prediciton results on new data:

#### Potential next steps include:

1. Plot the final model
2. Compress analysis report (please feel free to stop here if you feel that the entire report is too detailed)
3. Peform PCA pre-processing and select "key" features to make the model more interpretable 
4. Evaluate other classification algorithms (e.g. SVMs, kNN, etc.) 

### Analysis: 
```{r set_global_options}
rm(list=ls())
set.seed(12345)
source("~/Dropbox/datascience/R/mydsutils.R")
source("~/Dropbox/datascience/R/myplot.R")
# Gather all package requirements here
suppressPackageStartupMessages(require(caret))
suppressPackageStartupMessages(require(plyr))
```

```{r set_global_options_wd, echo=FALSE}
setwd("~/Documents/Work/Courses/Coursera/jhu-datascience/H-Practical-Machine-Learning/Project/HAR-WLE")
script_df <- data.frame(chunk_label="import_data", chunk_step_major=1, chunk_step_minor=0)
```

#### Step ``r script_df[nrow(script_df), "chunk_step_major"]``: import data
```{r import_data, cache=TRUE}
entity_df <- myimport_data(
    "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", 
    print_diagn=FALSE)
predict_df <- myimport_data(
    "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", 
    print_diagn=FALSE)

script_df <- rbind(script_df, 
                   data.frame(chunk_label="inspect_data", 
                              chunk_step_major=max(script_df$chunk_step_major)+1, chunk_step_minor=0))
```

#### Step ``r script_df[nrow(script_df), "chunk_step_major"]``: inspect data
```{r inspect_data_1, cache=TRUE}
#print(str(entity_df))
#print(str(predict_df))

# List info gathered for various columns
# X:                    int:    1:20            [record_id]
# user_name:            factor: 6 subject names

# raw_timestamp_part_1: int:                    [excluded for modeling]
# raw_timestamp_part_2: int:                    [excluded for modeling]
# cvtd_timestamp:       factor:                 [excluded for modeling]

# new_window:           factor: no; yes         [only no in predict_df]
# num_window:           int:    1:19622         

# classe:               factor: 5 categories    [response variable]
# problem_id:           int:    1:20            [only in predict_df]

#print(summary(entity_df))
print(summary(predict_df))
response_varname <- "classe"
```
Many columns in new data (predict_df) are missing for all records. Let's delete these columns from both training & new datasets to make the analysis faster & easier.

```{r inspect_data_2, cache=TRUE}
all_na_cols <- myfind_all_na_cols_df(predict_df)
print("deleting all na cols:")
print(all_na_cols)

predict_cln_df <- mydelete_cols_df(predict_df, all_na_cols)
print(summary(predict_cln_df))

# get rid of the original datasets to save memory
predict_df <- predict_cln_df
entity_df <- mydelete_cols_df(entity_df, all_na_cols)
print(summary(entity_df))

print(myplot_histogram(entity_df, "classe", fill_col_name="user_name"))
print(myplot_histogram(predict_df, "user_name", show_stats=FALSE))

script_df <- rbind(script_df, 
                   data.frame(chunk_label="select_features", 
                              chunk_step_major=max(script_df$chunk_step_major)+1, chunk_step_minor=0))                              
```

#### Step ``r script_df[nrow(script_df), "chunk_step_major"]``: select features
```{r select_features_1, cache=TRUE}
features_lst <- names(predict_df)

# Remove features that are not relevant for modeling
features_lst <- features_lst[!(features_lst %in% 
                                c("X", "user_name", "problem_id"))]
features_lst <- features_lst[!(features_lst %in% 
                                grep("timestamp", features_lst, value=TRUE))]

feats_df <- nearZeroVar(entity_df[, features_lst], saveMetrics=TRUE)
feats_df$feature <- rownames(feats_df)
print(subset(feats_df, nzv == TRUE))
nearZeroVars <- subset(feats_df, nzv == TRUE)$feature
print(summary(entity_df[, nearZeroVars]))
print(summary(predict_df[, nearZeroVars]))

feats_df <- subset(feats_df, nzv == FALSE)
```

Let's add a random variable (my.rnorm) as a feature since some models (e.g. random forests) like a random variable & any model that categorizes this feature as significant would be suspect.

```{r select_features_2, cache=TRUE}
random_varname <- "my.rnorm"
predict_df$my.rnorm <- rnorm(nrow(predict_df))
entity_df$my.rnorm <- rnorm(nrow(entity_df))
feats_df[nrow(feats_df) + 1, "feature"] <- random_varname

script_df <- rbind(script_df, 
                   data.frame(chunk_label="partition_data", 
                              chunk_step_major=max(script_df$chunk_step_major)+1, chunk_step_minor=0))                              
```

#### Step ``r script_df[nrow(script_df), "chunk_step_major"]``: partition data
```{r partition_data, cache=TRUE}
print(table(entity_df[, response_varname], entity_df$user_name))

inTrain <- createDataPartition(y=entity_df[, response_varname], p=0.8, list=FALSE)
train_df <- entity_df[inTrain, c(feats_df$feature, response_varname, "user_name")]
test_df <- entity_df[-inTrain, c(feats_df$feature, response_varname, "user_name")]

inValidate <- createDataPartition(y=train_df[, response_varname], p=0.8, list=FALSE)
train_df <- train_df[inValidate, c(feats_df$feature, response_varname, "user_name")]
validate_df <- train_df[-inValidate, c(feats_df$feature, response_varname, "user_name")]
print(table(train_df$classe, train_df$user_name))
print(table(validate_df$classe, validate_df$user_name))
print(table(test_df$classe, test_df$user_name))

script_df <- rbind(script_df, 
                   data.frame(chunk_label="select_features", 
                              chunk_step_major=max(script_df$chunk_step_major)+1, chunk_step_minor=0))                              
```

#### Step ``r script_df[nrow(script_df), "chunk_step_major"]``: select features
```{r design_models_1, cache=TRUE}
# Compute feature correlations with response to prioritize feature selection for model design
feats_cor_df <- data.frame( response.cor=cor(train_df[, feats_df$feature], 
                            unclass(train_df[, response_varname])))
feats_cor_df$response.cor.abs <- abs(feats_cor_df$response.cor)
feats_cor_df$feature <- rownames(feats_cor_df)
feats_cor_df <- orderBy(~ -response.cor.abs, data=feats_cor_df)

intersect(names(feats_df), names(feats_cor_df))
feats_tmp_df <- merge(feats_df, feats_cor_df)
feats_tmp_df <- orderBy(~ -response.cor.abs, data=feats_tmp_df)
print(head(feats_tmp_df))
print(head(feats_cor_df))

feats_all_df <- feats_tmp_df

# Extract features that have higher correlation than the random variable that was inducted ('my.rnorm')
feats_df <- subset(feats_all_df, 
    response.cor.abs >= feats_all_df[which(feats_all_df$feature == random_varname),
                                    "response.cor.abs"])
myprint_df(feats_df)

script_df <- rbind(script_df, 
                   data.frame(chunk_label="design_models", 
                              chunk_step_major=max(script_df$chunk_step_major)+1, chunk_step_minor=0))                              
```

#### Step ``r script_df[nrow(script_df), "chunk_step_major"]``: design models
```{r design_models, cache=TRUE}

# Create a function to fit a model based on different tuning parameters which will be used to optimize fit elapsed time vs. validation data prediction accuracy
mytrain_model <- function(par_row, trg_df=train_df) {
    #print(sprintf("num_feats: %d", num_feats))
    #print(sprintf("p_obs: %f", par_row$p_obs))
    
    # Because of random forests, let's try to make each fit run as similar as possible
    set.seed(12345)
    
    mdl_frmla <- reformulate(feats_df$feature[1:par_row$num_feats], response_varname)
    train_sst_df <- train_df[createDataPartition(y=train_df[, response_varname], 
                                                 p=par_row$p_obs, list=FALSE), 
                             c(response_varname, feats_df$feature[1:par_row$num_feats])]
    ctrl <- trainControl(method=par_row$sampling_method
                        , number=par_row$n_resample
                        #, repeats=repeats
                        #, seeds = seeds
                        )
    
    # mtry is a tuning parameter for random forests
    tune_grid <- expand.grid(mtry=par_row$mtry)
    mdl <- train(mdl_frmla, data=train_sst_df, 
                preProcess=eval(parse(text=par_row$preProcess)),
                method="rf", tuneGrid=tune_grid
                , trControl = ctrl
                )
    #print(mdl)
    #print(varImp(mdl))
    
    # collect fitting results data
    df <- data.frame(
                    num_feats=par_row$num_feats,
                    p_obs=par_row$p_obs,
                    preProcess=par_row$preProcess,
                    sampling_method=par_row$sampling_method,
                    n_resample=nrow(mdl$resample),
                    mtry=mdl$results[nrow(mdl$results), "mtry"],
                    accuracy_train=mdl$results[nrow(mdl$results), "Accuracy"],
                    accuracy_valdt=sum((validate_df$classe == 
                                        predict(mdl, newdata=validate_df)) * 1) / 
                                    nrow(validate_df),                                    
                    elapsed=mdl$times$everything["elapsed"])
    rownames(df) <- par_row$mdl_name
    
    # this took a long time to figure out. model object from caret package cannot
    #   be coerced into the return data frame. Please feel free to suggest a 
    #   beter method to handle multiple return objects 
    return(list(fit_mdl=mdl, fit_df=df))
}

# Run this process several times to 
#   maximize feature selection, 
#   maximize utilization of all training data observations
#   maximize accuracy of predictions on validation data
#   minimize elapsed time of fitting
#   The constraints at this step:
#       elapsed time < 60 secs

par_grid <- data.frame(mdl_name='base', num_feats=nrow(feats_df) / 2, p_obs=0.20, 
                       preProcess='NULL', sampling_method="cv", n_resample=10, 
                       mtry=3, stringsAsFactors=FALSE)
par_grid[2:7, ] <- par_grid[1, ]

par_grid[2, "mdl_name"] <- "num_feats=all";         par_grid[2, "num_feats"] <- nrow(feats_df)
par_grid[3, "mdl_name"] <- "p_obs=0.30";            par_grid[3, "p_obs"] <- 0.30
par_grid[4, "mdl_name"] <- "preProcess=center;scl"; par_grid[4, "preProcess"] <- 'c("center", "scale")'
par_grid[5, "mdl_name"] <- "sampling_method=boot";  par_grid[5, "sampling_method"] <- 'boot'
par_grid[6, "mdl_name"] <- "n_resample=15";         par_grid[6, "n_resample"] <- 15
par_grid[7, "mdl_name"] <- "mtry=6";                par_grid[7, "mtry"] <- 6

#par_grid[2:7, "n_resample"] <- 10
#par_grid[2:7, "sampling_method"] <- 'cv'
print(par_grid)

models_df <- data.frame()
for (row in 1:nrow(par_grid)) {
    print(sprintf("row: %d", row))
    attach(mytrain_model(par_grid[row, ]))
    mdl <- fit_mdl; head(predict(mdl, newdata=validate_df)); #print(mdl)
    models_df <- rbind(models_df, fit_df); #print(models_df)
    if (row == 1) mdl_1 <- mdl
    if (row == 2) mdl_2 <- mdl
    if (row == 3) mdl_3 <- mdl
    if (row == 4) mdl_4 <- mdl    
    if (row == 5) mdl_5 <- mdl    
    if (row == 6) mdl_6 <- mdl    
    if (row == 7) mdl_7 <- mdl    
}

models_df$id <- rownames(models_df)
models_df$accuracy_valdt_ix <- models_df$accuracy_valdt * 100.0 / models_df$accuracy_valdt[1]
models_df$elapsed_ix <- models_df$elapsed * 100.0 / models_df$elapsed[1]
models_df <- mutate(models_df, color=ifelse((id == "base"), "black", "NavyBlue"))
print(orderBy(~ - elapsed_ix + accuracy_valdt_ix, data=models_df))
# print(myplot_scatter(models_df, "elapsed_ix", "accuracy_valdt_ix") + 
#         geom_text(aes(label=id, color=color), size=3) + 
#         theme(legend.position="none"))
print(ggplot(models_df, aes(x=elapsed_ix, y=accuracy_valdt_ix)) +
        geom_point(aes(size=elapsed), color="grey") +   
        geom_text(aes(label=id, color=color), size=4) + 
        theme(legend.position="none"))

sel_mdl <- mdl_1
print(sel_mdl)
print(confusionMatrix(validate_df[, response_varname], 
                      predict(sel_mdl, newdata=validate_df)))
print(varImp(sel_mdl))

script_df <- rbind(script_df, 
                   data.frame(chunk_label="fit_all_train", 
                              chunk_step_major=max(script_df$chunk_step_major)+1, chunk_step_minor=0))                              
```

#### Step ``r script_df[nrow(script_df), "chunk_step_major"]``: fit all training obs
```{r fit_all_train_validate, cache=TRUE}
par_row <- par_grid[1, ]; par_row$mdl_name <- 'train_all'; par_row$p_obs <- 1.0; 
print(par_row)

attach(mytrain_model(par_row, trg_df=rbind(train_df, validate_df)))
train_all_mdl <- fit_mdl; 
print(head(predict(train_all_mdl, newdata=validate_df)))
print(train_all_mdl)
print(varImp(train_all_mdl))
train_all_models_df <- fit_df
print(train_all_models_df)

```
### Step ??: fit model for all training & validation data
#```{r fit_model_train_validate, cache=TRUE}
par_grid <- data.frame(mdl_name='train_validate_all', num_feats=nrow(feats_df) / 2, p_obs=1.00, preProcess='NULL', 
                        sampling_method="cv", n_resample=10, mtry=3, stringsAsFactors=FALSE)
print(par_grid)

attach(train_model(par_grid[1, ]))
train_validate_all_mdl <- fit_mdl; 
print(head(predict(train_validate_all_mdl, newdata=test_df)))
print(train_validate_all_mdl)
print(varImp(train_validate_all_mdl))
train_validate_all_models_df <- fit_df)
print(train_validate_all_models_df)

```

### Step 11: predict results for new data
#```{r predict_newdata_1, cache=TRUE}
predict_df[, paste0("classe.predict.", "a_benchmark_trg_sst_mdl")] <-
             predict(a_benchmark_trg_sst_mdl, predict_df)
print(predict_df[, c("X", "user_name", 
                     paste0("classe.predict.", "a_benchmark_trg_sst_mdl"))])
```

### Step 12: export prediction for new data
#```{r export_prediction_1, cache=TRUE}
export_pml_files <- function(x, dir_name) {

    if (!file.exists(paste0("./export"))) dir.create("export")
    if (!file.exists(paste0("./export/", dir_name))) 
        dir.create(paste0("./export/", dir_name))
    
    for (i in 1:length(x)) {
        filename <- paste0("./export/", dir_name, "/problem_id_", i, ".txt")
        write.table(x[i], file=filename, quote=FALSE, 
                    row.names=FALSE, col.names=FALSE)
    }
}
export_pml_files(predict_df[, paste0("classe.predict.", 
                                     "a_benchmark_trg_sst_mdl")], 
                 "a_benchmark_trg_sst_mdl")
```

### Step ##: build training, validation & test datasets
#```{r build_datasets_1, cache=TRUE}
```

```{r print_sessionInfo, echo=FALSE}
sessionInfo()
```