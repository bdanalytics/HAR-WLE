---
# Get YAML keywords from myYAML_ref.Rmd
title: "Human Activity Recognition (HAR): Weight Lifting Exercises (WLE) Classification"
author: "bdanalytics"
#output: html_document
---
#### Date: `r format(Sys.time(), "(%a) %b %d, %Y")`

Data: Measurement of exercises performed by six male participants aged between 20-28 years, with little weight lifting experience using a light dumbbell (1.25kg)  
Source: http://groupware.les.inf.puc-rio.br/har  
Time period: 

### Synopsis:

How you built your model:

How you used cross validation:

What you think the expected out of sample error is:

Why you made the choices you did:

Use your prediction model to predict 20 different test cases:

Potential next steps include:

### Analysis 
```{r set_global_options}
rm(list=ls())
set.seed(12345)
source("~/Dropbox/datascience/R/mydsutils.R")
source("~/Dropbox/datascience/R/myplot.R")
# Gather all package requirements here
suppressPackageStartupMessages(require(caret))
```

```{r set_global_options_wd, echo=FALSE}
setwd("~/Documents/Work/Courses/Coursera/jhu-datascience/H-Practical-Machine-Learning/Project/HAR-WLE")
```

### Step 01: import data
```{r import_data, cache=TRUE}
entity_df <- myimport_data(
    "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", 
    print_diagn=FALSE)
predict_df <- myimport_data(
    "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", 
    print_diagn=FALSE)
```

#### Step 02.1: inspect data
```{r inspect_data_1, cache=TRUE}
#print(str(entity_df))
#print(str(predict_df))

# List info gathered for various columns
# X:                    int:    1:20            [record_id]
# user_name:            factor: 6 subject names

# raw_timestamp_part_1: int:                    [excluded for modeling ?]
# raw_timestamp_part_2: int:                    [excluded for modeling ?]
# cvtd_timestamp:       int:                    [excluded for modeling ?]

# new_window:           factor: no; yes         [only no in predict_df]
# num_window:           int:    1:19622         [excluded for modeling ?]

# classe:               factor: 5 categories    [prediction variable]
# problem_id:           int:    1:20            [only in predict_df]

#print(summary(entity_df))
print(summary(predict_df))
```
Many columns in test dataset (predict_df) have missing data for all records. Let's delete these columns from both training & test datasets to make the analysis faster & easier.

```{r inspect_data_2, cache=TRUE}
all_na_cols <- myfind_all_na_cols_df(predict_df)
print("deleting all na cols:")
print(all_na_cols)

predict_cln_df <- mydelete_cols_df(predict_df, all_na_cols)
print(summary(predict_cln_df))

# get rid of the original datasets to save memory
predict_df <- predict_cln_df
entity_df <- mydelete_cols_df(entity_df, all_na_cols)
print(summary(entity_df))

# print(myplot_histogram(entity_df[sample(1:nrow(entity_df), 20), ], "classe", 
#                        fill_col_name="user_name"))
# print(myplot_histogram(entity_df[sample(1:nrow(entity_df), 20), ], "classe", 
#                        fill_col_name="user_name", show_stats=FALSE))
print(myplot_histogram(entity_df, "classe", fill_col_name="user_name"))
print(myplot_histogram(predict_df, "user_name", show_stats=FALSE))

# print(myplot_scatter(predict_df, "new_window", "num_window", 
#                      colorcol_name="user_name"))
# print(myplot_scatter(entity_df, "new_window", "num_window", 
#                      colorcol_name="user_name"))
# print(myplot_scatter(entity_df, "new_window", "num_window", 
#                      colorcol_name="classe"))
#pairs(subset(entity_df, select=-c(col_symbol)))

# Create new features that help diagnostics
#   Convert factors to dummy variables
#   Build splines   require(splines); bsBasis <- bs(training$age, df=3)
```

### Step 03: extract features
```{r extract_features_2, cache=TRUE}
features_lst <- names(predict_df)
features_lst <- features_lst[!(features_lst %in% 
                                c("X", "user_name", "problem_id"))]
## remove nearZeroVar features (not much variance)
#require(reshape)
#var_features_df <- melt(summaryBy(. ~ factor(0), data=entity_df[, features_lst], 
#                             FUN=var, keep.names=TRUE), 
#                             variable_name=c("feature"))
#names(var_features_df)[2] <- "var"
#print(var_features_df[order(var_features_df$var), ])
# summaryBy ignores factors whereas nearZeroVar inspects factors
print(nearZeroVar(entity_df[, features_lst], saveMetrics=TRUE))
nearZeroVars <- features_lst[nearZeroVar(entity_df[, features_lst])]
#print(var(entity_df[, nearZeroVars]))
#print(var(entity_df[, features_lst]))
print(summary(entity_df[, nearZeroVars]))
print(summary(predict_df[, nearZeroVars]))

features_lst <- features_lst[!(features_lst %in% nearZeroVars)]
```

Let's add a random variable (my.rnorm) as a feature since some models (e.g. random forests) need a random varaible & any model that categorizes this feature as significant would be suspect.  
```{r extract_features_4, cache=TRUE}
predict_df$my.rnorm <- rnorm(nrow(predict_df))
entity_df$my.rnorm <- rnorm(nrow(entity_df))
features_lst <- c(features_lst, "my.rnorm")
```

### Step 04: build training, validation & test datasets
```{r build_datasets_2, cache=TRUE}
print(table(entity_df$classe, entity_df$user_name))
# k_fold <- 5
# entity_df[order(entity_df$classe, 
#                   entity_df$user_name, 
#                   entity_df$my.rnorm),"my.cv_ix"] <- 
#     rep(1:k_fold, length.out=nrow(entity_df))
# summaryBy(X ~ my.cv_ix, data=entity_df, FUN=length)
# tapply(entity_df$X, list(entity_df$classe, entity_df$user_name, 
#                            entity_df$my.cv_ix), length)

#require(DAAG)
#entity_df$classe.proper <- as.numeric(entity_df$classe == "A")
#rnorm.glm <- glm(classe.proper ~ rnorm, family=binomial, data=entity_df)
#cv.binary(rnorm.glm, nfolds=k_fold, print.details=TRUE)
#result <- cv.lm(df=entity_df, form.lm=formula(classe ~ rnorm), 
#                    m=k_fold, seed=12345, printit=TRUE)

inTrain <- createDataPartition(y=entity_df$classe, p=0.8, list=FALSE)
train_df <- entity_df[inTrain, c(features_lst, "classe", "user_name")]
test_df <- entity_df[-inTrain, c(features_lst, "classe", "user_name")]
print(table(train_df$classe, train_df$user_name))
print(table(test_df$classe, test_df$user_name))
```

### Step 07: design models
```{r design_models_1, cache=TRUE}

num_feats <- 2
p_obs <- 0.01
preProcess <- c("center", "scale")
sampling_method <- "boot"

fit_time_df <- data.frame()
for (num_feats in seq(4, 4, 2)) {
    print(sprintf("num_feats:%d", num_feats))
    mdl_sst_formula <- reformulate(features_lst[1:num_feats], "classe")
    train_sst_df <- train_df[createDataPartition(y=train_df$classe, p=p_obs, 
                                                 list=FALSE), ]
    ctrl <- trainControl(method=sampling_method
                         #, repeats = 5
                         #, seeds = seeds
                         )
    mdl <- train(mdl_sst_formula, data=train_sst_df, 
                                           preProcess=preProcess, 
                                           method="rf"
                 , trControl = ctrl
                 )
    #print(mdl)
    #fit_time_df <- data.frame()
    
    fit_time_df <- rbind(fit_time_df, 
                         data.frame(num_feats=num_feats,
                                    p_obs=p_obs,
                                    preProcess=paste(preProcess, collapse=";"),
                                    sampling_method=sampling_method,
                                    n_resample=nrow(mdl$resample),
                                    mtry=mdl$results[nrow(mdl$results), "mtry"],
                                    accuracy=mdl$results[nrow(mdl$results), "Accuracy"],
                                    elapsed=mdl$times$everything["elapsed"]))
}
print(fit_time_df)

#print(mdl_sst_formula)
#print(table(train_sst_df$classe, train_sst_df$user_name))
#print(head(train_sst_df))
# a_benchmark_trg_sst_mdl <- train(mdl_sst_formula, data=train_sst_df, 
#                                  preProcess=c("center", "scale"), 
#                                  method="rf")

test_df[, paste0("classe.predict.", "a_benchmark_trg_sst_mdl")] <-
            predict(a_benchmark_trg_sst_mdl, test_df)
print(confusionMatrix(test_df[, "classe"], 
                      test_df[, paste0("classe.predict.", 
                                       "a_benchmark_trg_sst_mdl")]))
```

### Step 11: predict results for new data
```{r predict_newdata_1, cache=TRUE}
predict_df[, paste0("classe.predict.", "a_benchmark_trg_sst_mdl")] <-
             predict(a_benchmark_trg_sst_mdl, predict_df)
print(predict_df[, c("X", "user_name", 
                     paste0("classe.predict.", "a_benchmark_trg_sst_mdl"))])
```

### Step 12: export prediction for new data
```{r export_prediction_1, cache=TRUE}
export_pml_files <- function(x, dir_name) {

    if (!file.exists(paste0("./export"))) dir.create("export")
    if (!file.exists(paste0("./export/", dir_name))) 
        dir.create(paste0("./export/", dir_name))
    
    for (i in 1:length(x)) {
        filename <- paste0("./export/", dir_name, "/problem_id_", i, ".txt")
        write.table(x[i], file=filename, quote=FALSE, 
                    row.names=FALSE, col.names=FALSE)
    }
}
export_pml_files(predict_df[, paste0("classe.predict.", 
                                     "a_benchmark_trg_sst_mdl")], 
                 "a_benchmark_trg_sst_mdl")
```

### Step ##: build training, validation & test datasets
```{r build_datasets_1, cache=TRUE}
```

```{r print_sessionInfo, echo=FALSE}
sessionInfo()
```